{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1. Tokenization.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNNYf/HJdTPlEOP+nY/w7ix"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##**1. Tokenization**\n","\n","\"I love my Dog\" - [001 002 003 004] & \"I love my Cat\" - [001 002 003 005] are similiar since theyr are about loving a pet\n","\n","\n","This process of encoding a sentence into numbers is called TOKENIZATION. \n","\n","The code snippet we use for this is"],"metadata":{"id":"BuCHMlO1UP3I"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LLa-lJ7xUH1a","executionInfo":{"status":"ok","timestamp":1648920461757,"user_tz":-360,"elapsed":4602,"user":{"displayName":"Md. Rezuwan Hassan","userId":"15497472128905455505"}},"outputId":"6768e8bf-e8d6-4bf6-f363-299970a6aa46"},"outputs":[{"output_type":"stream","name":"stdout","text":["{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n","\n"," Note: The tokenizer is smart enough to deduct all the signs\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","\n","sentences = ['I love my dog','I love my cat', 'You love my dog!'] \n","\n","tokenizer = Tokenizer(num_words = 100) #num_words = The number of maximum most frequent words to keep\n","tokenizer.fit_on_texts(sentences) #Telling the tokenizer to go through all the text and fit itself into them like this\n","word_index = tokenizer.word_index #The full lit of words\n","print(word_index)\n","\n","print('\\n','Note: The tokenizer is smart enough to deduct all the signs')"]},{"cell_type":"code","source":[""],"metadata":{"id":"3TVS90_RUUtC"},"execution_count":null,"outputs":[]}]}